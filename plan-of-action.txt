# igraph_clusters

The actual trace segmentation and graph building code is done. What needs to
be done now is build clusters (using one of igraph's community structure algos)
and then use them in iterative mapping.

These are the next two things on my TODO in this regard:

1.  The trace segmentation algorithm.

    Our goal here is to build segments such that every mapped read is
    completely contained within some segment. The current code does not work at
    all with CAGE, because update_CAGE_trace_expectation_from_location only
    updates the trace at the promoter position (instead of across the entire
    fragment). The trace segmentation algorithm therefore does not know enough
    to build trace segments that perfectly contain each mapping.

    I have added a check to the graph building code that fails if a mapped read
    is found that does not perfectly fit into a trace. You can see it fail by
    running test_iterative_mapping in tests.py. 

    The current code *should* work with Chipseq, since
    update_chipseq_trace_expectation_from_location updates the entire fragment
    in the trace. However, the test is still failing. I believe this is
    actually pretty close to working, and is failing for a different reason. It
    is probably a boundary condition or maybe a rounding/normalization error.
    Unlike the CAGE, where the overlap between mapped reads and trace segments
    looks pretty random, these always have segments that are *just 1-3bp short*
    of some mapped read. Here is some example output from 3 runs of the test
    (`simulate_chipseq.py`):

        Trace segmentation failed: found mapped read fragment (4389, 5014) and trace segment (0, 5012)
        Trace segmentation failed: found mapped read fragment (4400, 5012) and trace segment (0, 5009)
        Trace segmentation failed: found mapped read fragment (4420, 5014) and trace segment (0, 5013)

    *note to self* - try to fix this before the end of the day today.

    We discussed some ideas to fix this yesterday, including developing an
    assay-agnostic "canonical value" representation of mapped reads for
    iterative mapping.

2.  Infrastructure for traces and iterative mapping

    At the moment, the current use of segmented traces is basically a hacked
    proof of concept to show that they work. The segmented aspect of the trace
    is not used in anyway, this just shows that I was able to drop-in replace
    the old traces with the new segmented traces and everything still works.

    This needs new design. Note that we are also inefficiently repeating
    build_segmented_trace for each iterative mapping sample. Actually determing
    the trace segments, building the graph, and clustering only needs to
    happen once for each trace/mapped_reads_db relevant to the assay.

    We have been discussing a vague idea for a design that, after clustering,
    would build new temporary mapped reads dbs and traces for each cluster,
    call the existing iterative mapping code (with minor modifications, but
    goal is to reuse as much as possible) for each temporary mapped reads db/
    trace.

    At the moment, all of the iterative mapping functions update the trace
    with generic functions meant to be called on a segmented_trace_t. One
    interesting idea might be to build a master trace that all of the clusters
    will update, and pass a list of pointers to trace segments in the master
    trace to the subfunctions and update those instead. This would minimize
    memory consumption, and since the clusters are all separate groups of
    segments, there is a lot of potential for concurrency here. Each trace
    segment is also protected by a mutex, so you can decide at which level
    concurrency should happen - should multiple threads update on a single
    trace segment (currently what happens, carried over from previous design),
    or should there be a thread processing each cluster? Lots of options.

# master

## RNASeq

There are two primary problems with the RNASeq.

1.  Mapping rate for mutated reads in test

    simulate_rnaseq.py passes on the test for perfect reads in a genome with
    introns when Statmap is using both the mismatch and the estimated error
    model. It fails on the tests for mutated reads regardless of error model.

    The tests fail because they were designed for perfect reads (to isolate
    potential problems with the intron finding code) and expect all reads that
    did not have probes overlapping an intron to map at least once. This needs
    to be relaxed for tests with mutated reads.

    However, even if we were to relax this assumption, the mapping rates for
    mutated reads look bad. For the mismatch error model we map about
    30% and for the estimated error model about 50%. Is this acceptable? The
    interaction between the mutated reads generated by the test data (currently
    using the same method using in test_error_rate_estimation.py) and the
    error model needs to be investigated.

2.  Performance on real data.

    Last time we tried to map real RNASeq (SRR023502.fastq on
    amold:/media/scratch/RNAseq_test) mapping was unacceptably slow. The result
    of my profiling work was that all the time is being spent on the index
    search, and the reason is because of min match penalties that are too low,
    which means Statmap spends an enormous amount of time running around in the
    index. The error model needs to be examined and adjusted to resolve this problem.

    Another reason why this problem may be occurring is the exceptional length
    of the RNASeq reads. They are 152 bps, much longer than anything we usually
    map. Although the index search is still working on indexed_seq_len bp fragments,
    the min match penalty calculation may be super low due to being calculated
    from the lenght of the whole read. I did a lot of work to try and sample the
    min match penalty from blocks of reads, for example, and to derive the index
    probe search penalty from the ratio of the expected value of an index probe,
    but nothing was a miraculous solution. These ideas all need to be
    considered in addition to any changes to the error model.

## Misc.

*   Sometimes the test suite will fail on test_fivep_sequence_finding(),
    usually saying it found 2 mappings when it expected one. I have *only* seen
    this happen on the first round of the test, where it is testing reads of
    length 15. I believe this happens because the reads are short enough that
    they just happen to be repeated in the randomly sampled genome. Probably
    the easiest way to fix this is just to increase the read length to reduce
    the probability of this happening.
*   The test suite occasionally fails on test_softclipped_read_finding, saying it
    found 99/100 mappings. I don't know why (was on my TODO).

## TODO

Everything else is in the TODO file.
